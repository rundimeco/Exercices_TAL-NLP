 \vspace{0.5cm}

\textbf{\textcolor{blue}{Attention, copier-coller à partir du PDF ne focntionnera pas, de nombreux caractères seront mal retranscrits créant ainsi un code non fonctionnel}}.

 Nous savons par la loi de Zipf que l'effectif des mots d'une langue suivait certaines lois de distribution.
 Nous pouvons exploiter ces propriétés pour calculer des "modèles de langue".
 Ces modèles de langue permettent d'identifier la langue d'un texte inconnu.

 Nous allons utiliser le \textbf{corpus parallèle} multilingue \textit{corpus\_multi.zip}
 Décompressez cette archive à l'endroit où vous avez positionné votre code (sans ajouter de dossier supplémentaire auquel cas les chemins montrés ici seraient systématiquement à modifier). Il y a un sous-dossier pour chaque langue (22 langues en tout) et pour chaque langue un corpus d'apprentissage ("appr") et un corpus de test ("test").
 Dans un nouveau script Python (notebook ou autre), testez le code suivant (en vous plaçant au bon endroit)\footnote{Attention, ainsi que noté dans le code le séparateur de dossier n'est pas le même sur Windows et sur Linux/Mac, il vous faudra adapter votre code.}:

\begin{python}
import glob
liste_fichiers = glob.glob("corpus_multi/*/*/*")
print("Nombre de fichiers : %i"%len(liste_fichiers)
for chemin in liste_fichiers:
  print(chemin)
  print(chemin.split("\\\\"))#Sur windows
  #NB: sur Windows on a plus rarement : chemin.split("\\")
  #print(chemin.split("/"))#Sur Linux/Mac
  break#stoppe le code (provisoire)
\end{python}

Observez le résultat du second print qui doit ressembler à :

\begin{python}
['corpus_multi', 'en', 'test', '2009-10-23_celex_IP-09-1574.en.html']
\end{python}

Ceci nous permettra d'exploiter la structure des dossiers pour faire des traitements efficaces.

\vspace{0.5cm}
\exer{Créer le modèle de langue (jeu de données appr)}
\vspace{0.5cm}

Nous allons avoir besoin d'ouvrir chaque fichier en mode texte, à partir de son chemin. Ce qui peut être fait avec la petite fonction suivante\footnote{
 NB: sur Linux ou MacOS, utf-8 est l'encoding par défaut donc on n'est pas obligé de le préciser. Par contre sur Windows, ne pas le mentionner peut poser problème.}

\begin{python}
def lire_fichier(chemin):
  with open(chemin, "r", encoding="utf-8") as f:
    chaine = f.read()
  return chaine
\end{python}

Nous allons traiter simplement le jeu de données d'apprentissage en changeant le démarrage de la boucle for :

\begin{python}
import glob
dic_langues = {}#pour recevoir les modèles de langue
liste_fichiers_appr = glob.glob("corpus_multi/*/appr/*")
print("Nombre de fichiers : %i"%len(liste_fichiers_appr)
for chemin in liste_fichiers_appr:
  #Ligne suivante à commenter ensuite pour soulager l'affichage
  print(chemin)
  dossiers = chemin.split("\\\\")#Sur Windows
  #dossiers = chemin.split("/")#Sur Linux/Mac
  langue = dossiers[1]
  ## on calcule les mots les plus frequents du texte:
  if langue not in dic_langues:
  ##on crée un sous-dictionnaire pour une nouvelle langue
    dic_langues[langue] = {}
  #on réutilise notre outillage de découpage en mots :
  chaine = lire_fichier(chemin)
  mots = chaine.split()
  # et on stocke les effectifs : 
  for m in mots:
    if m not in dic_langues[langue]:
      dic_langues[langue][m] = 1
    else:
      dic_langues[langue][m] += 1
  #Ligne suivante à commenter ensuite pour soulager l'affichage
  print(dic_langues)
  break# on termine la boucle
\end{python}

Une fois que vous avez observé le code ci-dessus, et qu'il fait le job, commentez les deux dernières lignes pour l'exécuter sur la totalité du corpus.

 Pour chaque langue du corpus, utilisez l'ensemble des textes figurant dans le dossier " appr " pour calculer un modèle de langue. Ce modèle de langue sera constitué des $n$ mots les plus fréquents identifiés dans ce sous-corpus.

Pour l'étape suivante, il faut \textbf{ commenter les deux print et le \texttt{break} } afin de ne pas surcharger l'affichage. Après la boucle on va pouvoir calculer les modèles de langue de la façon suivante :
\begin{python}
dic_modeles = {}
for langue, dic_effectifs in dic_langues.items():
  paires=[[effectif,mot] for mot,effectif in  dic_effectifs.items()]
  liste_tri = sorted(paires)[-10:]#les 10 mots fréquents
  dic_modeles[langue] = [mot for effectif, mot in liste_tri]
print(dic_modeles.keys())#liste de slangues du dictionnaire
\end{python}

NB: s'il n'y a pas plus de une langue dans dic\_modeles (print ci-dessus) c'est qu'il y a un problème dans la boucle.

 Sauvegardez tous ces modèles dans un unique fichier au format \textsc{Json} :

\begin{python}
import json
#ici on change de mode d'ouverture du fichier:
with open("models.json", "w", encoding="utf-8") as w:
  w.write(json.dumps(dic_modeles, indent = 2, ensure_ascii=False))
\end{python}

Consultez ce fichier \texttt{models.json} pour comprendre le modèle.

\vspace{0.5cm}
\exer{Appliquer les modèles de langue}
\vspace{0.5cm}

 Nous allons maintenant regarder si ces modèles fonctionnent bien.
 Pour chaque langue nous allons nous intéresser aux fichiers contenus dans le dossier  "test".
 Nous allons comparer les 10 mots les plus fréquents de chacun de ces fichiers avec nos modèles de langue.
 Nous allons ainsi pouvoir établir un diagnostic de langue pour chaque texte.

 Il nous faut donc obtenir pour chaque texte du dossier "test" la même \textbf{représentation} afin de pouvoir le comparer aux modèles que nous avons calculé.

 Dans une nouvelle cellule, on va tout d'abord charger les modèles :

\begin{python}
with open("models.json", "r", encoding="utf-8") as f:
  dic_modeles = json.load(f)
\end{python}

Puis nous allons parcourir les fichiers de test, les ouvrir et compter l'effectif des mots de ce texte :

\begin{python}
liste_fichiers_test = glob.glob("corpus_multi/*/test/*")
print("Nombre de fichiers : %i"%len(liste_fichiers_test)
for chemin in liste_fichiers_test:
  dossiers = chemin.split("\\\\")#Sur Windows
  #dossiers = chemin.split("/")#Sur Linux/Mac
  langue = dossiers[1]
  chaine = lire_fichier(chemin)
  mots = chaine.split()
  dic_freq_texte = {}
  for m in mots:
    if m not in dic_freq_texte:
      dic_freq_texte[m] = 1
    else:
      dic_freq_texte[m] += 1
  print(dic_freq_texte)
  # dic_freq_texte contient les effectifs des mots du texte
  break
\end{python}

Maintenant on enlève le \texttt{break}, les \texttt{print} inutiles et on calcule les dix mots les plus fréquents du texte : 

\begin{python}
for chemin in liste_fichiers_test:
  dossiers = chemin.split("\\\\")#Sur Windows
  #dossiers = chemin.split("/")#Sur Linux/Mac
  langue = dossiers[1]
  chaine = lire_fichier(chemin)
  mots = chaine.split()
  dic_freq_texte = {}
  for m in mots:
    if m not in dic_freq_texte:
      dic_freq_texte[m] = 1
    else:
      dic_freq_texte[m] += 1
  paires=[[effectif,mot] for mot,effectif in  dic_freq_texte.items()]
  liste_tri = sorted(paires)[-10:]#les 10 mots fréquents
  plus_frequents = set([mot for effectif, mot in liste_tri])
  print(plus_frequents)
  break
\end{python}
%  paires = [[effectif, mot] for mot, effectif in  dic_effectifs.items()]
%  liste_tri = sorted(paires)[-10:]#les 10 mots fréquents
%  dic_modeles[langue] = set([mot for effectif, mot in liste_tri])

À nouveau, on enlève le \texttt{break}, les print inutiles et on passe à la suite. Ici on va calculer l'\textbf{intersection} (les mots en commun) entre les mots fréquents du texte et ceux de chaque modèle de langue :

\begin{python}
for chemin in liste_fichiers_test:
  dossiers = chemin.split("\\\\")#Sur Windows
  #dossiers = chemin.split("/")#Sur Linux/Mac
  langue = dossiers[1]
  chaine = lire_fichier(chemin)
  mots = chaine.split()
  dic_freq_texte = {}
  for m in mots:
    if m not in dic_freq_texte:
      dic_freq_texte[m] = 1
    else:
      dic_freq_texte[m] += 1
  paires=[[effectif,mot] for mot,effectif in  dic_freq_texte.items()]
  liste_tri = sorted(paires)[-10:]#les 10 mots fréquents
  plus_frequents = set([mot for effectif, mot in liste_tri])

  print("Document en %s"%langue)
  for langue_ref, model in dic_models.items():
    mots_communs = set(model).intersection(plus_frequents)
    print("%i mots en commun avec le modèle (%s):"
%(len(mots_communs), langue_ref))
    print(mots_communs)
  break
\end{python}

Maintenant on va chercher automatiquement l'intersection la plus grande et vérifier si ça correspond à notre intuition. On finit de compléter notre boucle:

\begin{python}
for chemin in liste_fichiers_test:
  dossiers = chemin.split("\\\\")#Sur Windows
  #dossiers = chemin.split("/")#Sur Linux/Mac
  langue = dossiers[1]
  chaine = lire_fichier(chemin)
  mots = chaine.split()
  dic_freq_texte = {}
  for m in mots:
    if m not in dic_freq_texte:
      dic_freq_texte[m] = 1
    else:
      dic_freq_texte[m] += 1
  paires=[[effectif,mot] for mot,effectif in  dic_freq_texte.items()]
  liste_tri = sorted(paires)[-10:]#les 10 mots fréquents
  plus_frequents = set([mot for effectif, mot in liste_tri])
  liste_predictions = []
  print("Document en %s"%langue) 
  for langue_ref, model in dic_models.items():
    mots_communs = set(model).intersection(plus_frequents)
    NB_mots_communs = len(mots_communs)
    liste_predictions.append([NB_mots_communs, langue_ref])
    print(sorted(liste_predictions, reverse=True)[:3])
\end{python}

A vous maintenant de faire la suite : extraire la prédiction majoritaire (la première qui apparaît avec la fonction \texttt{sorted}\footnote{Par souci de simplicité, on ne fait pas d'exception pour les cas où deux langues arriveraient \textit{ex-aequo} }) puis passer à la section suivante : l'évaluation.

\vspace{0.5cm}
\exer{Evaluer le diagnostic}
\vspace{0.5cm}

 Nous devons évaluer l'efficacité de notre système : pour chaque langue d'une part et pour l'ensemble des langues d'autre part.
 Nous allons considérer que notre but est de trouver la bonne classe (i.e. la bonne langue) pour chaque document.

Dans un premier temps nous utiliserons l'\textbf{exactitude} c'est à dire la proportion de bonnes réponses. Il suffira pour chaque texte du jeu de \texttt{test} de comparer le diagnostic de langue donné par votre programme et de regarder s'il est conforme à la vérité. Voici les étapes :

 \begin{itemize}
 \item On crée une variable qui va stocker le nombre de réponses (c'est donc un entier qui vaudra zéro au démarrage), appelons la \texttt{NB\_bonnes\_reponses}
 \item Pour chaque texte du jeu de \texttt{test}, on prédit la langue du texte
 \item On compare la langue prédite avec la langue réelle :
 \begin{itemize}
 \item Si c'est pareil, alors \texttt{NB\_bonnes\_reponses} est \textbf{incrémenté} de 1
 \item Sinon il ne se passe rien
 \end{itemize}
 \item À la fin on calcule la proportion de bonnes réponses : \texttt{NB\_bonnes\_reponses}/ Le nombre de documents du jeu de test.
 \end{itemize}


Dans un deuxième temps, nous utiliserons le Rappel qui permet de vérifier que l'on a bien classé tous les documents d'une même langue ainsi que la Précision qui évalue combien de documents sont classés dans la bonne langue. 
 Il s'agit donc de calculer le nombre de Vrais Positifs (VP), Faux Positifs (FP) et Faux Négatifs (FN) \textbf{et ce pour chaque langue}.

Pour chaque document traité à l'exercice précédent nous allons comptabiliser nos VP, nos FP et nos FN.
 Nous pourrons ainsi pour chaque langue calculer :
\begin{itemize}
\item le rappel : VP/(VP+FN)
\item la précision : VP/(VP+FP)
\item la F$_1$-mesure (moyenne harmonique du rappel et de la précision) : (2*Rappel*Précision) / (Précision+Rappel)
\end{itemize}

Cela signifie que l'on aura un dictionnaire dans lequel pour chaque langue on comptabilisera le nombre de VP, FP et FN. Sa structure sera donc de la forme suivante:
\begin{python}
dic_resultats= {
  "fr":{"VP":0, "FP":0, "FN":0},
  "en":{"VP":0, "FP":0, "FN":0}
  #.... et ainsi de suite
}
\end{python}

\textbf{Astuce} vous pouvez ajouter les langues de manière dynamique dans la boucle en ajoutant au fur et à mesure les langues que vous traitez, en schématique : 

\begin{python}

dic_resultats= {}
#....

for chemin in liste_fichiers_test:
  # ici le code pour avoir "langue" la langue du document
  # ici le code pour avoir "langue_pred" la langue predite
  # setdefault(cle, valeur) cree une valeur par defaut pour cle 
  dic_resultats.setdefault(langue, {"VP":0, "FP":0, "FN":0})
  dic_resultats.setdefault(langue_pred, {"VP":0, "FP":0, "FN":0})
\end{python}

Ensuite, vous comparerez les résultats selon le nombre $n$ de mots fréquents pris en considération et vous identifierez les erreurs les plus fréquentes: couples de langues qui posent problème et documents pour lesquelles la prédiction en plus d'être fausse est étrange (normalement vous avez un document en bulgare pour lequel la prédiction est "anglais", il faudra aller voir pourquoi).

\textbf{Vous verrez que pour se faciliter la tâche, on aura intérêt à en faire des fonctions :}

\begin{itemize}
  \item fonction \texttt{predire\_langue} qui prend en entrée un texte et retourne la langue prédite pour ce texte
  \item focntion \texttt{evaluer} qui prend en entrée une liste de prédictions de langues, la liste des langues réelles et calcule exactitude, rappel, précision et F-mesure 
\end{itemize}

\vspace{0.5cm}
\exer{Et en caractères?}
\vspace{0.5cm}

\begin{itemize}
  \item Reprenez les 3 exercices précédents en utilisant non plus des mots mais des $n$-grammes de caractères.
  \item Testez pour des valeurs de $n$ de 1 à 4 en créant une fonction qui prend en paramètre le nombre de caractères (le $N$)
  \item Regardez si cela fonctionne mieux ou moins bien que la méthode en mots
\end{itemize}

L'évaluation de cette partie du travail sera réalisée en cours le 28/10 (présence obligatoire).

Vous rédigerez 2 pages décrivant vos résultats à l'issue de la séance.


\vspace{0.5cm}
\exer{Exercice bonus : avec du \textit{Machine Learning} }
\vspace{0.5cm}


 En apprentissage automatique on cherche le bon algorithme qui permet d'associer des exemples (ici des textes) à des classes ou étiquettes (ici des langues).
Il s'agit de vectoriser les exemples et d'entraîner un classifieur à trouver la relation entre le contenu des exemples (souvent regroupés dans une matrice nommée $X$) et les classes (une liste nommée $y$). On va stocker séparément ces élements pour l'apprentissage (ou \texttt{train}) et le \texttt{test}.

 L'exemple donné ci-dessous exploite la bibliothèque \texttt{sklearn} (à éventuellement installer via la commande \texttt{pip install sklearn}). Nous exploitons le vectoriseur \textsc{CountVectorizer}
 \footnote{\url{https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html}}, les classifieur bayésiens naïfs 
 \footnote{\url{https://scikit-learn.org/stable/modules/naive_bayes.html}}
 et les rapports de classification
 \footnote{\url{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html}}.

\subsection*{I: Préparation des données}

On va stocker les textes et les langues dans des structures de données adaptées :

\begin{python}
from sklearn.feature_extraction.text import CountVectorizer
import glob
import re
textes = {"appr":[], "test":[]}
classes= {"appr":[], "test":[]}

for path in glob.glob("corpus_multi/*/*/*")[:1500]:
  _, lang, corpus, filename = re.split("/", path)
  classes[corpus].append(lang)
  with open(path, encoding="utf-8") as f:
    chaine = f.read()

  textes[corpus].append(chaine)
\end{python}

A noter que l'on ne prend que les 1500 premiers exemples seulement pour faire fonctionner le programme sur un échantillon.

\subsection*{II: Vectorisation et stockage des étiquettes}
 
 Par simplicité, on utilise les 1.000 caractéristiques (\textit{features}) les plus fréquentes sur tout le corpus (et donc pas les plus fréquentes pour chaque langue).

\begin{python}
vectorizer = CountVectorizer(max_features=1000)
# Pour travailler avec des caractères : analyzer="char"
# spécifier la taille des n-grammes : ngram_range=(min,max))
X_train = vectorizer.fit_transform(textes["appr"]).toarray()
X_test  = vectorizer.transform(textes["test"]).toarray()
y_train = classes["appr"]
y_test  = classes["test"]
\end{python}

\subsection*{III: Classification et évaluation}

\begin{python}
from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB()
y_pred = gnb.fit(X_train, y_train).predict(X_test)
NB_textes  = X_test.shape[0]
NB_erreurs = y_test != y_pred).sum()
print("Erreurs d'étiquetage sur %d textes : %d"%(NB_textes, NB_erreurs))

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))
\end{python}

Votre travail consiste à faire varier les paramètres de \textsc{CountVectorizer} :
\begin{itemize}
  \item Tester différentes valeurs pour le nombre de caractéristiques : \texttt{max\_features}\footnote{Pas de valeur trop grande pour ne pas surcharger la mémoire}
  \item Tester en vectorisant avec des N-grammes de caractères : \texttt{analyzer="char"}
  \item Faire varier les valeurs min et max de N:  \texttt{ngram\_range=(min,max)}
  \item Quel est la configuration la plus efficace ?
  \item A votre avis pourquoi ?
\end{itemize}
